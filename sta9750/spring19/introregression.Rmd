---
title: "Linear regression with `R` "
author: "Víctor Peña"
output:
  html_document:
    toc: true
    theme: cosmo
---



---
# Commenting out some stuff
---

---
# Don't print messages, errors, warnings
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(error = FALSE, message = FALSE, warning = FALSE)
```

# Reviews of Italian restaurants in Manhattan


```{r, echo = FALSE}
nyc = read.csv("~/spring19/STA9750/nyc.csv")
```

We'll analyze a dataset in Sheather (2009) that has information about 150 Italian restaurants in Manhattan that were open in 2001 (some of them are closed now). The variables are:

* `Case`: case-indexing variable 

* `Restaurant`: name of the restaurant

* `Price`: average price of a meal and a drink 

* `Food`: average [Zagat](http://www.zagat.com) rating of the quality of the food (from 0 to 25)

* `Decor`: same as above, but with quality of the decor 

* `Service`: same as above, but with quality of service

* `East`: it is equal to `1` if the restaurant is on the East Side (i.e. east of Fifth Ave)

In our analysis, the response variable will be `Price`. 

## Exploratory data analysis

The function `pairs` creates a scatterplot matrix:

```{r}
pairs(nyc[,-c(1,2)])
```

I wrote `nyc[,-c(1,2)]` instead of `nyc` so that the first two variables, `Case` and `Restaurant`, are not plotted.

We can get quick and dirty summaries of the variables with `summary`:

```{r}
summary(nyc[,-c(1,2)])
```

The function `ggpairs` in `library(GGally)` produces the equivalent plot, but with `ggplot2`:

```{r, message=FALSE}
library(GGally)
ggpairs(nyc[,-c(1,2)])
```

Do you see any interesting patterns?

## Fitting regression models with the `lm` function

Fitting regression models with `R` is easy. For example, we can fit a model where the outcome is `Price` and the predictors are `Food`, `Decor`, `Service`, and `East` with the code

```{r}
mod = lm(Price ~ Food + Decor + Service + East, data = nyc)
```

Calling the object `mod` only gives us coefficients:

```{r}
mod
```

If we want $p$-values, $R^2$, and more, we can get them with `summary()`:

```{r}
summary(mod)
```

We can get diagnostic plots by `plot`ting the model. That will give us 4 diagnostic plots. We can arrange them in a figure with 2 rows and 2 columns with `par(mfrow=c(2,2))`:

```{r}
par(mfrow=c(2,2))
plot(mod)
```

If, for some reason, we want them in 1 row and 4 columns:

```{r}
par(mfrow=c(1,4))
plot(mod)
```

The instruction `par(mfrow=c(<rows>, <columns>)` isn't specific to "models". We can use it to arrange figures with multiple rows and columns of plots in `library(graphics)`. Unfortunately, it doesn't work with `ggplot2`. The analogue instruction for `ggplot2` is `grid.arrange` (see `ggplot2` handout for examples).

We can extract diagnostics from `mod`. For example, if we want to extract Cook's distances and plot them against observation number, we can use:

```{r}
cookd = cooks.distance(mod)
plot(cookd)
```

Other useful functions are `hatvalues` (for leverages), `residuals` (for residuals), and `rstandard` (for standardized residuals).

## Automatic model selection

### Backward, forward, and stepwise

Backward selection with AIC:
```{r}
step(mod, direction='backward')
```

If we want to do forward selection, we have to give a starting model and a bigger model that contains the all the variables that we might want to include in our model.

```{r}
nullmod = lm(Price ~ 1, data = nyc) # no variables
fwd = step(nullmod, 
           scope=list(upper=mod), 
           direction='forward')
```

In the code above, the starting point was a model with no variables (`nullmod`) and the model that included the variables under consideration is `mod` (which contains `Food`, `Service`, `Decor`, and `East`).

We can do forward selection starting with a model that has some variable(s) already. For example, we can start with a model that has `Service` already in.
```{r}
mod2 = lm( Price ~ Service, data = nyc)
fwd = step(mod2, 
           scope=list(upper=mod), 
           direction='forward')
```

We can do stepwise regression with `direction = 'both'`. In stepwise regression, variables can get in or out of the model. We can specify the smallest and biggest model in our search with `scope`. For example, if we want to start our stepwise search with a model has `Service` as a predictor and we want to restrict our search to models that include `Service` and potentially include all the other predictors:

```{r}
mod2 = lm( Price ~ Service, data = nyc)
fwd = step(mod2, 
           scope=list(lower=mod2, upper=mod), 
           direction='both')
```

We can change our selection criterion to BIC by adding the command `k = log(number of observations)`. For example,

```{r}
n = nrow(nyc) 
step(mod, direction='backward', k = log(n))
```

### All subsets selection with `library(leaps)`

If we want to find the "best" model given a set of predictors according to BIC or adjusted $R^2$, `library(leaps)` is helpful. For example, if we want to consider all models that might contain `Food`, `Decor`, `Service`, and `East`:

```{r}
library(leaps)
allsubs = regsubsets(Price ~ Food + Decor + Service + East, data = nyc)
```

We can see the best models with 1, 2, 3, and 4 predictors using the `summary` function:
```{r}
summary(allsubs)
```
If we restrict ourselves to all models that have, say, **exactly** 2 predictors, the "best" models according to AIC, BIC, and adjusted $R^2$ will coincide: it will be the model with 2 predictors that has the smallest residual sum of squares. The overall "best" model will be one of the 4 "best" models for a fixed number of predictors. AIC and BIC need not agree on the overall best model, because they penalize model sizes differently (the penalty for AIC is smaller, which favors bigger models). 

We can visualize the BICs of the "best" models (the model at the top of the plot is the best model overall):

```{r}
plot(allsubs)
```

We can also visualize the adjusted $R^2$s:

```{r}
plot(allsubs, scale = 'adjr')
```

I haven't found a way to get AICs, unfortunately. 

## Prediction

```{r, echo = FALSE}
nyctest = read.csv('~/spring19/nyctest.csv')
```


The dataset `nyctest` has data for some Italian restaurants that weren't included in `nyc`. Let's see how well we predict the prices of the meals. We'll use the following model

```{r}
mod = lm(Price ~ Food + Decor + East, data = nyc)
```

We can find point predictions and 99\% prediction intervals as follows:

```{r}
preds = predict(mod, newdata = nyctest, interval = 'prediction', level = 0.99)
preds
```

Let's compare them to the actual prices:

```{r}
compare = cbind(preds, nyctest$Price)
```

We can find a column that tests whether the prediction interval contain the prices:

```{r}
test = (compare[,4]>=compare[,2]) & (compare[,4] <= compare[,3])
cbind(compare, test)
```

We can find the proportion of intervals that trap the true price as

```{r}
mean(test)
```

**Sources:**

* Sheather, Simon. *A modern approach to regression with R.* Springer Science & Business Media, 2009.

* [Multiple and logistic regression](https://www.datacamp.com/courses/multiple-and-logistic-regression), Datacamp course by [Ben Baumer](http://www.science.smith.edu/~bbaumer/w/). 


